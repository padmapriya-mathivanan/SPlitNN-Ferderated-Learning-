<b>Split network approach on Federated learning with Image data and communication efficient quantization</b>
<p align=justify>Friction in data sharing is a large challenge for large scale machine learning. Recently techniques such as Federated Learning, Differential Privacy and Split Learning aim to address this data sharing with unstructured data, privacy and regulation of data sharing. new decentralised learning methodology called, ‘Split Neural Networks’ is a new technique developed at the MIT Media Lab’s Camera Culture group that allows for participating entities to train machine learning models without sharing any raw data. The training of a neural network (NN) is ‘split’ across two or more hosts. The Aim of this internship is to achieve split neural network on image data and perform Quantization on neural network for efficient computation and reduced communication cost on data processing.</p>
![image](https://user-images.githubusercontent.com/57839087/116559487-196e0900-a933-11eb-91eb-a4444e0ca8ad.png)

MIT's work on split neural network replication. Split neural network is to split the network across identifying a cut layer. The different split acts as client and server in the federated training.



