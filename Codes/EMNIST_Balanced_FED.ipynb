{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EMNIST_Balanced_FED.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU9lX-sg9fBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2b6d44-50b4-45b6-cb2a-f74d1823ab9f"
      },
      "source": [
        "pip install keras-tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tqdm\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5c/ac63c65b79a895b8994474de2ad4d5b66ac0796b8903d60cfea3f8308d5c/keras_tqdm-2.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-tqdm) (2.4.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tqdm) (4.41.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-tqdm) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-tqdm) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-tqdm) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-tqdm) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-tqdm) (1.15.0)\n",
            "Installing collected packages: keras-tqdm\n",
            "Successfully installed keras-tqdm-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM80m7afBtJ5"
      },
      "source": [
        "import numpy as np\r\n",
        "import random\r\n",
        "import cv2\r\n",
        "import os\r\n",
        "from imutils import paths\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Conv2D\r\n",
        "from tensorflow.keras.layers import MaxPooling2D\r\n",
        "from tensorflow.keras.layers import Activation\r\n",
        "from tensorflow.keras.layers import Flatten\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.optimizers import SGD\r\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfaLsZ01B2uq"
      },
      "source": [
        "\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "#from keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "from keras import layers\r\n",
        "from keras.layers import *\r\n",
        "from keras.utils import np_utils\r\n",
        "\r\n",
        "from keras_tqdm import TQDMNotebookCallback"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmonCiH5CJZ_",
        "outputId": "437008e7-9758-4d0d-83c2-1a22171367c5"
      },
      "source": [
        "#ignore warnings in the output\r\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n",
        "\r\n",
        "from tensorflow.python.client import device_lib\r\n",
        "\r\n",
        "# Check all available devices if GPU is available\r\n",
        "print(device_lib.list_local_devices())\r\n",
        "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 1099927324619996221\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15703311680\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 702575247030263237\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFACLXJ8CVQR",
        "outputId": "c35c8166-6b86-41d5-a798-7ea6ae13e88b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws6EjfsmCZGV"
      },
      "source": [
        "train = pd.read_csv('/content/gdrive/MyDrive/Internship/emnist/emnist-balanced-train.csv')\r\n",
        "test = pd.read_csv('/content/gdrive/MyDrive/Internship/emnist/emnist-balanced-test.csv')\r\n",
        "mapping = pd.read_csv(\"/content/gdrive/MyDrive/Internship/emnist/emnist-balanced-mapping.txt\", \r\n",
        "                      delimiter = ' ', \r\n",
        "                      index_col=0, \r\n",
        "                      header=None, \r\n",
        "                      squeeze=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvzMo66JweNh",
        "outputId": "28f8d2f3-5639-40c1-d4b3-a5993dec7b42"
      },
      "source": [
        "train.shape, test.shape, mapping.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((112799, 785), (18799, 785), (47,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9qcDBmuCl09"
      },
      "source": [
        "X_train, y_train = train.iloc[:, 1:], train.iloc[:, 0]\r\n",
        "X_test, y_test = test.iloc[:, 1:], test.iloc[:, 0]\r\n",
        "\r\n",
        "del train\r\n",
        "del test"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBLkW-hz2SnN",
        "outputId": "45d43411-2142-4ee6-e9f0-e3c6059948e2"
      },
      "source": [
        "classes = np.unique(y_train)\r\n",
        "num_classes = len(classes)\r\n",
        "num_classes"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRn06gwOw7rt",
        "outputId": "caa79927-af85-4763-de0c-bf07eba2f8fa"
      },
      "source": [
        "# Convert the images into 3 channels\r\n",
        "X_train=np.dstack([X_train] * 3)\r\n",
        "X_test=np.dstack([X_test]*3)\r\n",
        "X_train.shape,X_test.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((112799, 784, 3), (18799, 784, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ez2P2ta2XnU",
        "outputId": "95a58d68-8853-4ba8-e85f-1c314c64036e"
      },
      "source": [
        "# Reshape images as per the tensor format required by tensorflow\r\n",
        "X_train = X_train.reshape(-1, 28,28,3)\r\n",
        "X_test= X_test.reshape (-1,28,28,3)\r\n",
        "X_train.shape,X_test.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((112799, 28, 28, 3), (18799, 28, 28, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DSVCfaN2U0v",
        "outputId": "52af56a7-f703-4e90-83aa-505702ef170e"
      },
      "source": [
        "# Resize the images 48*48 as required by VGG16\r\n",
        "from keras.preprocessing.image import img_to_array, array_to_img\r\n",
        "X_train = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in X_train])\r\n",
        "X_test = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in X_test])\r\n",
        "#train_x = preprocess_input(x)\r\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((112799, 48, 48, 3), (18799, 48, 48, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udo8m-e4CplJ"
      },
      "source": [
        "# Normalise the data and change data type\r\n",
        "X_train = X_train / 255.\r\n",
        "X_test = X_test / 255.\r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_test = X_test.astype('float32')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j59_cY7vC5JO"
      },
      "source": [
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "# Converting Labels to one hot encoded format\r\n",
        "y_train_one_hot = to_categorical(y_train)\r\n",
        "y_test_one_hot = to_categorical(y_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-abjMM_gC8Yn"
      },
      "source": [
        "# partition to train and val\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, \r\n",
        "                                                  y_train, \r\n",
        "                                                  test_size= 0.10, \r\n",
        "                                                  random_state=88)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lm-Rm3bC-2q",
        "outputId": "aaeda87f-dd8d-458d-b05b-cc84401c7e5d"
      },
      "source": [
        "# Finally check the data size whether it is as per tensorflow and VGG16 requirement\r\n",
        "X_train.shape,X_val.shape,y_train.shape,y_val.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((101519, 48, 48, 3), (11280, 48, 48, 3), (101519,), (11280,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrIahRoE2xdi"
      },
      "source": [
        "# Define the parameters for instanitaing VGG16 model. \r\n",
        "IMG_WIDTH = 48\r\n",
        "IMG_HEIGHT = 48\r\n",
        "IMG_DEPTH = 3\r\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCEvyjSn200t"
      },
      "source": [
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "import os, time\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "#from keras.datasets import fashion_mnist\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import keras\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D, MaxPooling2D\r\n",
        "from keras.layers import Dense, Dropout, Flatten\r\n",
        "#from keras.layers.advanced_activations import LeakyReLU\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from keras.applications import VGG16;\r\n",
        "from keras.applications.vgg16 import preprocess_input\r\n",
        "import os"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2BaRRnT22zG"
      },
      "source": [
        "# Preprocessing the input \r\n",
        "X_train = preprocess_input(X_train)\r\n",
        "X_val = preprocess_input(X_val)\r\n",
        "X_test = preprocess_input(X_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6epTgp3524l3",
        "outputId": "8ca7cc7c-c67a-4e51-c0f0-e1b3305dcd72"
      },
      "source": [
        "#  Create base model of VGG16\r\n",
        "conv_base = VGG16(weights='imagenet',\r\n",
        "                  include_top=False, \r\n",
        "                  input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH)\r\n",
        "                 )\r\n",
        "conv_base.summary()\r\n",
        "\r\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 48, 48, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 48, 48, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 48, 48, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 24, 24, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 24, 24, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 12, 12, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 6, 6, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un1JoYqn31K7",
        "outputId": "eeca2e27-b96c-487b-d93d-b563d62e5e81"
      },
      "source": [
        "# Extracting features\r\n",
        "train_features = conv_base.predict(np.array(X_train), batch_size=BATCH_SIZE, verbose=1)\r\n",
        "test_features = conv_base.predict(np.array(X_test), batch_size=BATCH_SIZE, verbose=1)\r\n",
        "val_features = conv_base.predict(np.array(X_val), batch_size=BATCH_SIZE, verbose=1)\r\n",
        "#for layer in conv_base.layers:\r\n",
        "#    layer.trainable = False"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6345/6345 [==============================] - 66s 6ms/step\n",
            "1175/1175 [==============================] - 7s 6ms/step\n",
            "705/705 [==============================] - 4s 6ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CftizniC4Qsb"
      },
      "source": [
        "# 6.1 Saving the features so that they can be used for future\r\n",
        "np.savez(\"train_features\", train_features, y_train)\r\n",
        "np.savez(\"test_features\", test_features, y_test)\r\n",
        "np.savez(\"val_features\", val_features, y_val)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX_fUh504UcU",
        "outputId": "40894f46-e845-40fa-a059-9f3f6de5fc41"
      },
      "source": [
        "# Current shape of features\r\n",
        "print(train_features.shape, \"\\n\",  test_features.shape, \"\\n\", val_features.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(101519, 1, 1, 512) \n",
            " (18799, 1, 1, 512) \n",
            " (11280, 1, 1, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UZHlZGW4WeV"
      },
      "source": [
        "# Flatten extracted features\r\n",
        "train_features_flat = np.reshape(train_features, (101519, 1*1*512))\r\n",
        "test_features_flat = np.reshape(test_features, (18799, 1*1*512))\r\n",
        "val_features_flat = np.reshape(val_features, (11280, 1*1*512))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYFPk4vQ4Yvs"
      },
      "source": [
        "from keras import models\r\n",
        "from keras.models import Model\r\n",
        "from keras import layers\r\n",
        "from keras import optimizers\r\n",
        "from keras import callbacks\r\n",
        "from keras.layers.advanced_activations import LeakyReLU\r\n",
        "\r\n",
        "# 7.0 Define the densely connected classifier followed by leakyrelu layer and finally dense layer for the number of classes\r\n",
        "NB_TRAIN_SAMPLES = train_features_flat.shape[0]\r\n",
        "NB_VALIDATION_SAMPLES = val_features_flat.shape[0]\r\n",
        "NB_EPOCHS = 100"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvqCEJsF9fWq",
        "outputId": "2528a216-cd80-46c7-b6c4-14db5fd0bd33"
      },
      "source": [
        "train_features_flat.shape, val_features_flat.shape, y_train.shape, y_val.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((101519, 512), (11280, 512), (101519,), (11280,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVgKrgO64ez7"
      },
      "source": [
        "class VGG16:\r\n",
        "    @staticmethod\r\n",
        "    def build(shape, classes):\r\n",
        "        model = models.Sequential()\r\n",
        "        model.add(layers.Dense(512, activation='relu', input_dim=(1*1*512)))\r\n",
        "        model.add(layers.LeakyReLU(alpha=0.1))\r\n",
        "        model.add(layers.Dense(47, activation='softmax'))\r\n",
        "        return model"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-KmBXja4tnC"
      },
      "source": [
        "y_train = to_categorical(y_train, 47)\r\n",
        "y_test = to_categorical(y_test, 47)\r\n",
        "y_val = to_categorical(y_val, 47)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke7ov-Z99uMI",
        "outputId": "9168600e-7efa-42ed-c246-a62aa4ce0515"
      },
      "source": [
        "y_train.shape, y_test.shape, y_val.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((101519, 47), (18799, 47), (11280, 47))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M8z9ssYDDy8"
      },
      "source": [
        "def create_clients(image_list, label_list, num_clients=5, initial='clients'):\r\n",
        "    ''' return: a dictionary with keys clients' names and value as \r\n",
        "                data shards - tuple of images and label lists.\r\n",
        "        args: \r\n",
        "            image_list: a list of numpy arrays of training images\r\n",
        "            label_list:a list of binarized labels for each image\r\n",
        "            num_client: number of fedrated members (clients)\r\n",
        "            initials: the clients'name prefix, e.g, clients_1 \r\n",
        "            \r\n",
        "    '''\r\n",
        "\r\n",
        "    #create a list of client names\r\n",
        "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\r\n",
        "\r\n",
        "    #randomize the data\r\n",
        "    data = list(zip(image_list, label_list))\r\n",
        "    random.shuffle(data)\r\n",
        "\r\n",
        "    #shard data and place at each client\r\n",
        "    size = len(data)//num_clients\r\n",
        "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\r\n",
        "\r\n",
        "    #number of clients must equal number of shards\r\n",
        "    assert(len(shards) == len(client_names))\r\n",
        "\r\n",
        "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2ReVqyxDNFV"
      },
      "source": [
        "#create clients\r\n",
        "clients = create_clients(train_features_flat, y_train, num_clients=10, initial='client')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbM1iP_fDSmX"
      },
      "source": [
        "def batch_data(data_shard, bs=32):\r\n",
        "    '''Takes in a clients data shard and create a tfds object off it\r\n",
        "    args:\r\n",
        "        shard: a data, label constituting a client's data shard\r\n",
        "        bs:batch size\r\n",
        "    return:\r\n",
        "        tfds object'''\r\n",
        "    #seperate shard into data and labels lists\r\n",
        "    data, label = zip(*data_shard)\r\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\r\n",
        "    return dataset.shuffle(len(label)).batch(bs)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2glQ1UzDXfa",
        "outputId": "63add59f-f351-4158-b3e0-6f26aadb56ae"
      },
      "source": [
        "#process and batch the training data for each client\r\n",
        "clients_batched = dict()\r\n",
        "for (client_name, data) in clients.items():\r\n",
        "    print(\"processing\")\r\n",
        "    clients_batched[client_name] = batch_data(data)\r\n",
        "    \r\n",
        "#process and batch the test set\r\n",
        "print(\"processing Test batches\")  \r\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((val_features_flat, y_val)).batch(len(y_val))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing\n",
            "processing\n",
            "processing\n",
            "processing\n",
            "processing\n",
            "processing\n",
            "processing\n",
            "processing\n",
            "processing\n",
            "processing\n",
            "processing Test batches\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s5QzYcNDl4L"
      },
      "source": [
        "lr = 0.01 \r\n",
        "comms_round = 100\r\n",
        "loss='categorical_crossentropy'\r\n",
        "metrics = ['accuracy']\r\n",
        "optimizer = SGD(lr=lr, \r\n",
        "                decay=lr / comms_round, \r\n",
        "                momentum=0.9\r\n",
        "               )              "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPLXrps2DoOt"
      },
      "source": [
        "def weight_scalling_factor(clients_trn_data, client_name):\r\n",
        "    client_names = list(clients_trn_data.keys())\r\n",
        "    #get the bs\r\n",
        "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\r\n",
        "    #first calculate the total training data points across clinets\r\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\r\n",
        "    # get the total number of data points held by a client\r\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\r\n",
        "    return local_count/global_count\r\n",
        "\r\n",
        "\r\n",
        "def scale_model_weights(weight, scalar):\r\n",
        "    '''function for scaling a models weights'''\r\n",
        "    weight_final = []\r\n",
        "    steps = len(weight)\r\n",
        "    for i in range(steps):\r\n",
        "        weight_final.append(scalar * weight[i])\r\n",
        "    return weight_final\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def sum_scaled_weights(scaled_weight_list):\r\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\r\n",
        "    avg_grad = list()\r\n",
        "    #get the average grad accross all client gradients\r\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\r\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\r\n",
        "        avg_grad.append(layer_mean)\r\n",
        "        \r\n",
        "    return avg_grad\r\n",
        "\r\n",
        "def test_model(test_features_flat, y_val,  model, comm_round):\r\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\r\n",
        "    #logits = model.predict(X_test, batch_size=100)\r\n",
        "    logits = model.predict(val_features_flat)\r\n",
        "    loss = cce(y_val, logits)\r\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(y_val, axis=1))\r\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\r\n",
        "    return acc, loss\r\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBWVctmwDr23",
        "outputId": "784a7fbd-ffe9-4ccc-e63d-72717b558dcb"
      },
      "source": [
        "#initialize global model\r\n",
        "smlp_global = VGG16()\r\n",
        "global_model = smlp_global.build(512,47)\r\n",
        "        \r\n",
        "#commence global training loop\r\n",
        "for comm_round in range(comms_round):\r\n",
        "            \r\n",
        "    # get the global model's weights - will serve as the initial weights for all local models\r\n",
        "    global_weights = global_model.get_weights()\r\n",
        "    \r\n",
        "    #initial list to collect local model weights after scalling\r\n",
        "    scaled_local_weight_list = list()\r\n",
        "\r\n",
        "    #randomize client data - using keys\r\n",
        "    client_names= list(clients_batched.keys())\r\n",
        "    random.shuffle(client_names)\r\n",
        "    \r\n",
        "    #loop through each client and create new local model\r\n",
        "    for client in client_names:\r\n",
        "        smlp_local = VGG16()\r\n",
        "        local_model = smlp_local.build(512,47)\r\n",
        "        local_model.compile(loss=loss, \r\n",
        "                      optimizer=optimizer, \r\n",
        "                      metrics=metrics)\r\n",
        "        \r\n",
        "        #set local model weight to the weight of the global model\r\n",
        "        local_model.set_weights(global_weights)\r\n",
        "        \r\n",
        "        #fit local model with client's data\r\n",
        "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\r\n",
        "        \r\n",
        "        #scale the model weights and add to list\r\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client)\r\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\r\n",
        "        scaled_local_weight_list.append(scaled_weights)\r\n",
        "        \r\n",
        "        #clear session to free memory after each communication round\r\n",
        "        K.clear_session()\r\n",
        "        \r\n",
        "    #to get the average over all the local model, we simply take the sum of the scaled weights\r\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\r\n",
        "    \r\n",
        "    #update global model \r\n",
        "    global_model.set_weights(average_weights)\r\n",
        "\r\n",
        "    #test global model and print out metrics after each communications round\r\n",
        "    for(test_features_flat, Y_val) in test_batched:\r\n",
        "        global_acc, global_loss = test_model(test_features_flat, Y_val, global_model, comm_round)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comm_round: 0 | global_acc: 1.826% | global_loss: 3.850151300430298\n",
            "comm_round: 1 | global_acc: 1.826% | global_loss: 3.8501527309417725\n",
            "comm_round: 2 | global_acc: 1.826% | global_loss: 3.8501534461975098\n",
            "comm_round: 3 | global_acc: 1.826% | global_loss: 3.850153684616089\n",
            "comm_round: 4 | global_acc: 1.826% | global_loss: 3.850154399871826\n",
            "comm_round: 5 | global_acc: 1.826% | global_loss: 3.850154399871826\n",
            "comm_round: 6 | global_acc: 1.826% | global_loss: 3.850154399871826\n",
            "comm_round: 7 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 8 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 9 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 10 | global_acc: 1.826% | global_loss: 3.8501548767089844\n",
            "comm_round: 11 | global_acc: 1.826% | global_loss: 3.8501548767089844\n",
            "comm_round: 12 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 13 | global_acc: 1.826% | global_loss: 3.8501548767089844\n",
            "comm_round: 14 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 15 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 16 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 17 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 18 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 19 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 20 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 21 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 22 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 23 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 24 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 25 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 26 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 27 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 28 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 29 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 30 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 31 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 32 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 33 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 34 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 35 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 36 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 37 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 38 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 39 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 40 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 41 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 42 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 43 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 44 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 45 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 46 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 47 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 48 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 49 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 50 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 51 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 52 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 53 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 54 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 55 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 56 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 57 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 58 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 59 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 60 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 61 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 62 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 63 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 64 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 65 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 66 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 67 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 68 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 69 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 70 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 71 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 72 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 73 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 74 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 75 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 76 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 77 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 78 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 79 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 80 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 81 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 82 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 83 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 84 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 85 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 86 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 87 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 88 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 89 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 90 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 91 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 92 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 93 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 94 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 95 | global_acc: 1.826% | global_loss: 3.8501555919647217\n",
            "comm_round: 96 | global_acc: 1.826% | global_loss: 3.850155830383301\n",
            "comm_round: 97 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 98 | global_acc: 1.826% | global_loss: 3.8501551151275635\n",
            "comm_round: 99 | global_acc: 1.826% | global_loss: 3.8501551151275635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1IH6_hlDu-a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}